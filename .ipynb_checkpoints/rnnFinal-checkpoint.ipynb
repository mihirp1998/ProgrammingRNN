{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import main\n",
    "import numpy as np\n",
    "from code import temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN( x,y,seqlen,vocab_size,seq_max_len):\n",
    "\tn_hidden = 50\n",
    "\tx_unstack = tf.unstack(x, seq_max_len, 1)\n",
    "\tweights = {\n",
    "\t    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]),name = 'w1')\n",
    "\t}\n",
    "\n",
    "\tbiases = {\n",
    "\t    'out': tf.Variable(tf.zeros([vocab_size]),name='b1')\n",
    "\t}\n",
    "\n",
    "\tlstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,name=\"lstm_cell\")\n",
    "\n",
    "\toutputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x_unstack, dtype=tf.float32,\n",
    "\t\t\t\t\t\t\t\tsequence_length=seqlen)\n",
    "\toutputs = tf.stack(outputs)\n",
    "\toutputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\tbatch_size = tf.shape(outputs)[0] \n",
    "\toutputs = tf.reshape(outputs, [-1, n_hidden])\n",
    "\toutputs  = tf.matmul(outputs, weights['out']) + biases['out']\n",
    "\t\n",
    "\tallOutputs = tf.nn.softmax(outputs)\n",
    "\tallOutputs = tf.reshape(allOutputs,[batch_size,seq_max_len,vocab_size],name=\"allOutputs\")\n",
    "\n",
    "\toutputs = tf.reshape(outputs,[batch_size,seq_max_len,vocab_size])\n",
    "\t\n",
    "\tindex = tf.range(0,batch_size)*seq_max_len + (seqlen -1)\n",
    "\toutputPred = tf.gather(tf.reshape(outputs,[-1,vocab_size]),index)\n",
    "\toutputPred = tf.nn.softmax(outputPred,name = \"predictedOutputs\")\n",
    "\toutputPred  = tf.argmax(outputPred,axis = -1)\n",
    "\tpredictedOutputs = []\n",
    "\toutputPred =  tf.one_hot(outputPred,vocab_size,name =\"one_hot\")\n",
    "\t# states = tf.identity(states,name=\"states\")\n",
    "\n",
    "\tfor i in range(20):\n",
    "        \t\tpredictedOutputs.append(outputPred)\n",
    "\t\toutputPred, states = lstm_cell(outputPred,states)\n",
    "\t\toutputPred = tf.matmul(outputPred, weights['out']) + biases['out']\n",
    "\t\t# softmax_cross_entropy_with_logits\n",
    "\t\toutputPred = tf.nn.softmax(outputPred,name =\"softymax\")\n",
    "\t\toutputPred  = tf.argmax(outputPred,axis = -1,name=\"argymax\")\n",
    "\t\toutputPred =  tf.one_hot(outputPred,vocab_size,name=\"one_hot_vec\")\n",
    "\n",
    "\n",
    "\n",
    "\tpredictedOutputs = tf.stack(predictedOutputs,name=\"predictedOutputing\")\t\n",
    "\n",
    "\n",
    "\treturn outputs,predictedOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainRun():\n",
    "\ttf.reset_default_graph()\n",
    "\tlearning_rate = 0.01\n",
    "\tx1,y1,t,vocab_size  = main.main()\n",
    "\tseq_max_len = max(t)\n",
    "\n",
    "\tx = tf.placeholder(\"float\", [None,seq_max_len, vocab_size],name=\"x\")\n",
    "\ty = tf.placeholder(\"float\", [None,seq_max_len, vocab_size],name= \"y\")\n",
    "\tseqlen = tf.placeholder(tf.int32, [None],name =\"seqlen\")\n",
    "\t\n",
    "\tpred,b = dynamicRNN(x,y,seqlen,vocab_size,max(t))\n",
    "\t# return pred,b\n",
    "\t# return pred,b\n",
    "\tinit = tf.global_variables_initializer()\n",
    "\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y),name=\"theloss\")\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,name=\"the_gdo\").minimize(cost,name=\"theoptimizer\")\n",
    "\tsaver = tf.train.Saver()\n",
    "\tsess = tf.Session()\n",
    "\t# saver = tf.train.import_meta_graph('saved/model-10000.meta')\n",
    "\t# saver.restore(sess,tf.train.latest_checkpoint('saved/.'))\n",
    "\n",
    "\tsess.run(init)\n",
    "\t# graph = tf.get_default_graph()\n",
    "\t# pred = graph.get_tensor_by_name(\"b1:0\")\n",
    "\t# print(sess.run(pred))\n",
    "\t# return\n",
    "\n",
    "\t# l,m  = sess.run([pred,b],feed_dict={x:x1,y:y1,seqlen:t})\n",
    "\t# return l,m,i\n",
    "\n",
    "\tfor i in range(10001):\n",
    "\t\t l,o  = sess.run([cost,optimizer],feed_dict={x:x1,y:y1,seqlen:t})\n",
    "\t\t if not(i % 500):\n",
    "\t\t\tprint(\"saving model\")\n",
    "\t\t \tsaver.save(sess,'./saved/SMallmodel',global_step = i)\n",
    "\t\t print(\"l {} o {} epoch {}\".format(l,o,i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainContinue():\n",
    "\ttf.reset_default_graph()\n",
    "\tlearning_rate = 0.01\n",
    "\tx1,y1,t,vocab_size  = main.main()\n",
    "\tsess = tf.Session()\n",
    "\tsaver = tf.train.import_meta_graph('saved/SMallmodel-10000.meta')\n",
    "\tsaver.restore(sess,tf.train.latest_checkpoint('saved/.'))\n",
    "\tgraph = tf.get_default_graph()\n",
    "\txP = graph.get_tensor_by_name(\"x:0\")\n",
    "\tyP = graph.get_tensor_by_name(\"y:0\")\n",
    "\tseqlenP = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "\n",
    "\t\n",
    "\tcost =graph.get_tensor_by_name(\"theloss:0\")\n",
    "\tb = graph.get_tensor_by_name(\"b1:0\")\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,name=\"the_gdo\").minimize(cost,name=\"theoptimizer\")\n",
    "\t\n",
    "\t\n",
    "\tfor i in range(10000,50000):\n",
    "\t\t l,_  = sess.run([cost,optimizer],feed_dict={xP:x1,yP:y1,seqlenP:t})\n",
    "\t\t # print(l,b)\n",
    "\t\t if not(i % 500):\n",
    "\t\t\tprint(\"saving model\")\n",
    "\t\t \tsaver.save(sess,'./saved/SMallmodel',global_step = i)\n",
    "\t\t print(\"l {}  epoch {}\".format(l,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore():\n",
    "\ttf.reset_default_graph()\n",
    "\ttoken_code = temp.getCode()\n",
    "\tx1,y1,t,vocab_size = main.testMain(token_code)\n",
    "\tx1,y1,t,vocab_size = main.main()\n",
    "\n",
    "\tword_to_id,id_to_word,token_docs = main.word_id()\n",
    "\tsess = tf.Session()\n",
    "\tx1 = np.zeros((1, max(t), vocab_size))\n",
    "\tidwod = word_to_id['import']\n",
    " \tx1[:,:,idwod] =  1\n",
    "\tt = [1]\n",
    "\tsaver = tf.train.import_meta_graph('saved/SMallmodel-10000.meta')\n",
    "\tsaver.restore(sess,tf.train.latest_checkpoint('saved/.'))\n",
    "\tgraph = tf.get_default_graph()\n",
    "\tpred = graph.get_tensor_by_name(\"predictedOutputing:0\")\n",
    "\tall_outputs  = graph.get_tensor_by_name(\"allOutputs:0\")\n",
    "\tall_outputs  = tf.argmax(all_outputs,axis = -1,name=\"AllOargymax\")\n",
    "\tall_outputs =  tf.one_hot(all_outputs,vocab_size,name=\"AllOone_hot_vec\")\n",
    "\tpredictedOutputs = []\n",
    "\t# for i in range(10):\n",
    "\t# \toutputPred, states = lstm_cell(outputsPred,states)\n",
    "\t# \toutputPred = tf.matmul(outputPred, weights['out']) + biases['out']\n",
    "\t# \t# softmax_cross_entropy_with_logits\n",
    "\t# \toutputPred = tf.nn.softmax(outputPred)\n",
    "\t# \tpredictedOutputs.append(outputPred)\n",
    "\t# \toutputPred  = tf.argmax(outputPred) \n",
    "\t# \toutputPred =  tf.one_hot(outputPred,vocab_size)\n",
    "\n",
    "\n",
    "\t# predictedOutputs = tf.stack(predictedOutputs,name=\"predictedOutputs\")\t\n",
    "\txP = graph.get_tensor_by_name(\"x:0\")\n",
    "\tyP = graph.get_tensor_by_name(\"y:0\")\n",
    "\tseqlenP = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "\tcost =graph.get_tensor_by_name(\"theloss:0\")\n",
    "\t# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=yP))\t\n",
    "\n",
    "\tp,cost,all_outputs  = sess.run([pred,cost,all_outputs],feed_dict={xP:x1,seqlenP:t,yP:y1})\n",
    "\tpred = np.argmax(p,axis = 2)\n",
    "\tpred = pred.flatten()\n",
    "\tpredArray = [id_to_word[i] for i in pred]\n",
    "\tpredWord = ' '.join(predArray)\n",
    "\taText = np.argmax(all_outputs,axis = 2)\n",
    "\taText = aText.flatten()\n",
    "\taText = [id_to_word[i] for i in aText]\n",
    "\taText = ' '.join(aText)\n",
    "\treturn p,aText,predWord,cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val', '12', 32)\n",
      "1\n",
      "('heck', 'import')\n",
      "1\n",
      "('heck', 'java')\n",
      "1\n",
      "('heck', '.')\n",
      "1\n",
      "('heck', 'util')\n",
      "1\n",
      "('heck', '.')\n",
      "1\n",
      "('heck', '*')\n",
      "1\n",
      "('heck', ';')\n",
      "1\n",
      "('heck', '\\n')\n",
      "2\n",
      "('heck', 'import')\n",
      "2\n",
      "('heck', 'java')\n",
      "2\n",
      "('heck', '.')\n",
      "2\n",
      "('heck', 'lang')\n",
      "2\n",
      "('heck', '.')\n",
      "2\n",
      "('heck', '*')\n",
      "2\n",
      "('heck', ';')\n",
      "2\n",
      "('heck', '\\n')\n",
      "3\n",
      "('heck', 'public')\n",
      "3\n",
      "('heck', 'class')\n",
      "3\n",
      "('heck', 'train1')\n",
      "3\n",
      "('heck', '\\n')\n",
      "4\n",
      "('heck', '{')\n",
      "4\n",
      "('heck', '\\n')\n",
      "5\n",
      "('heck', 'static')\n",
      "5\n",
      "('heck', 'ArrayList')\n",
      "5\n",
      "('heck', '<')\n",
      "5\n",
      "('heck', 'Integer')\n",
      "5\n",
      "('heck', '>')\n",
      "5\n",
      "('heck', 'array')\n",
      "5\n",
      "('heck', ';')\n",
      "5\n",
      "('heck', '\\n')\n",
      "6\n",
      "('heck', 'public')\n",
      "6\n",
      "('heck', 'static')\n",
      "6\n",
      "('heck', 'void')\n",
      "6\n",
      "('heck', 'main')\n",
      "6\n",
      "('heck', '(')\n",
      "6\n",
      "('heck', 'String')\n",
      "6\n",
      "('heck', 'args')\n",
      "6\n",
      "('heck', '[')\n",
      "6\n",
      "('heck', ']')\n",
      "6\n",
      "('heck', ')')\n",
      "6\n",
      "('heck', '\\n')\n",
      "7\n",
      "('heck', '{')\n",
      "7\n",
      "('heck', '\\n')\n",
      "8\n",
      "('heck', 'array')\n",
      "8\n",
      "('heck', '=')\n",
      "8\n",
      "('heck', 'new')\n",
      "8\n",
      "('heck', 'ArrayList')\n",
      "8\n",
      "('heck', '<')\n",
      "8\n",
      "('heck', 'Integer')\n",
      "8\n",
      "('heck', '>')\n",
      "8\n",
      "('heck', '(')\n",
      "8\n",
      "('heck', ')')\n",
      "8\n",
      "('heck', ';')\n",
      "8\n",
      "('heck', '\\n')\n",
      "9\n",
      "('heck', 'int')\n",
      "9\n",
      "('heck', 'fact')\n",
      "9\n",
      "('heck', '=')\n",
      "9\n",
      "('heck', 'factorial')\n",
      "9\n",
      "('heck', '(')\n",
      "9\n",
      "('heck', '5')\n",
      "9\n",
      "('heck', ')')\n",
      "9\n",
      "('heck', ';')\n",
      "9\n",
      "('heck', '\\n')\n",
      "10\n",
      "('heck', 'int')\n",
      "10\n",
      "('heck', 'multiply')\n",
      "10\n",
      "('heck', '=')\n",
      "10\n",
      "('heck', 'multiply')\n",
      "10\n",
      "('heck', '(')\n",
      "10\n",
      "('heck', '5')\n",
      "10\n",
      "('heck', ',')\n",
      "10\n",
      "('heck', '10')\n",
      "10\n",
      "('heck', ')')\n",
      "10\n",
      "('heck', ';')\n",
      "10\n",
      "('heck', '\\n')\n",
      "11\n",
      "('heck', 'binaryConver')\n",
      "11\n",
      "('heck', '(')\n",
      "11\n",
      "('heck', '8')\n",
      "11\n",
      "('heck', ')')\n",
      "11\n",
      "('heck', ';')\n",
      "11\n",
      "('heck', '\\n')\n",
      "12\n",
      "('heck', 'for')\n",
      "12\n",
      "('heck', '(')\n",
      "12\n",
      "('heck', 'int')\n",
      "12\n",
      "('heck', 'i')\n",
      "12\n",
      "('heck', '=')\n",
      "12\n",
      "('heck', '0')\n",
      "12\n",
      "('heck', ';')\n",
      "12\n",
      "('heck', 'i')\n",
      "12\n",
      "('heck', '<')\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "12\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "13\n",
      "not happening\n",
      "14\n",
      "not happening\n",
      "14\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "15\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "16\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "17\n",
      "not happening\n",
      "18\n",
      "not happening\n",
      "18\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "19\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "20\n",
      "not happening\n",
      "21\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "22\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "23\n",
      "not happening\n",
      "24\n",
      "not happening\n",
      "24\n",
      "not happening\n",
      "25\n",
      "not happening\n",
      "25\n",
      "not happening\n",
      "25\n",
      "not happening\n",
      "25\n",
      "not happening\n",
      "26\n",
      "not happening\n",
      "26\n",
      "not happening\n",
      "27\n",
      "not happening\n",
      "28\n",
      "not happening\n",
      "28\n",
      "not happening\n",
      "29\n",
      "not happening\n",
      "('words', ['import', 'java', '.', 'util', '.', '*', ';', '\\n', 'import', 'java', '.', 'lang', '.', '*', ';', '\\n', 'public', 'class', 'train1', '\\n', '{', '\\n', 'static', 'ArrayList', '<', 'Integer', '>', 'array', ';', '\\n', 'public', 'static', 'void', 'main', '(', 'String', 'args', '[', ']', ')', '\\n', '{', '\\n', 'array', '=', 'new', 'ArrayList', '<', 'Integer', '>', '(', ')', ';', '\\n', 'int', 'fact', '=', 'factorial', '(', '5', ')', ';', '\\n', 'int', 'multiply', '=', 'multiply', '(', '5', ',', '10', ')', ';', '\\n', 'binaryConver', '(', '8', ')', ';', '\\n', 'for', '(', 'int', 'i', '=', '0', ';', 'i', '<'])\n",
      "./code/train/train1.java\n",
      "./code/train/train1.java\n"
     ]
    }
   ],
   "source": [
    "\ttf.reset_default_graph()\n",
    "\ttoken_code = temp.getCode()\n",
    "\tx1,y1,t,vocab_size = main.testMain(token_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./code/train/train1.java\n",
      "./code/train/train1.java\n"
     ]
    }
   ],
   "source": [
    "x1,y1,t,vocab_size = main.main()\n",
    "word_to_id,id_to_word,token_docs = main.word_id()\n",
    "sess = tf.Session()\n",
    "x1 = np.zeros((1, max(t), vocab_size))\n",
    "idwod = word_to_id['import']\n",
    "x1[:,:,idwod] =  1\n",
    "t = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved/./SMallmodel-10000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph('saved/SMallmodel-10000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('saved/.'))\n",
    "graph = tf.get_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = graph.get_tensor_by_name(\"predictedOutputing:0\")\n",
    "all_outputs  = graph.get_tensor_by_name(\"allOutputs:0\")\n",
    "all_outputs  = tf.argmax(all_outputs,axis = -1,name=\"AllOargymax\")\n",
    "all_outputs =  tf.one_hot(all_outputs,vocab_size,name=\"AllOone_hot_vec\")\n",
    "predictedOutputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xP = graph.get_tensor_by_name(\"x:0\")\n",
    "yP = graph.get_tensor_by_name(\"y:0\")\n",
    "seqlenP = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "cost =graph.get_tensor_by_name(\"theloss:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tp,cost,all_outputs  = sess.run([pred,cost,all_outputs],feed_dict={xP:x1,seqlenP:t,yP:y1})\n",
    "\tpred = np.argmax(p,axis = 2)\n",
    "\tpred = pred.flatten()\n",
    "\tpredArray = [id_to_word[i] for i in pred]\n",
    "\tpredWord = ' '.join(predArray)\n",
    "\taText = np.argmax(all_outputs,axis = 2)\n",
    "\taText = aText.flatten()\n",
    "\taText = [id_to_word[i] for i in aText]\n",
    "\taText = ' '.join(aText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
